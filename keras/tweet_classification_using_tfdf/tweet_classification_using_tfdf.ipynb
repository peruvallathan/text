{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTTWZY3ouKkw"
   },
   "source": [
    "# Text classification using Decision Forests and pretrained embeddings\n",
    "\n",
    "**Author:** Gitesh Chawda<br>\n",
    "**Date created:** 09/05/2022<br>\n",
    "**Last modified:** 09/05/2022<br>\n",
    "**Description:** Using Tensorflow Decision Forests for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e_ffJ7euKky"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "[TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests) (TF-DF)\n",
    "is a collection of state-of-the-art algorithms for Decision Forest models that are\n",
    "compatible with Keras APIs. The module includes Random Forests, Gradient Boosted Trees,\n",
    "and CART, and can be used for regression, classification, and ranking tasks.\n",
    "\n",
    "In this example we will use Gradient Boosted Trees with pretrained embeddings to\n",
    "classify disaster-related tweets.\n",
    "\n",
    "### See also:\n",
    "\n",
    "- [TF-DF beginner tutorial](https://www.tensorflow.org/decision_forests/tutorials/beginner_colab)\n",
    "- [TF-DF intermediate tutorial](https://www.tensorflow.org/decision_forests/tutorials/intermediate_colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BhPAeCPuKkz"
   },
   "source": [
    "Install Tensorflow Decision Forest using following command :\n",
    "`pip install tensorflow_decision_forests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1r0B_f5uKkz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GwlicScLuKkz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow Decision Forests 1.4.0 is compatible with the following TensorFlow Versions: ['2.12.0']. However, TensorFlow 2.9.3 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n",
      "WARNING:root:TensorFlow Decision Forests 1.4.0 is compatible with the following TensorFlow Versions: ['2.12.0']. However, TensorFlow 2.9.3 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n",
      "WARNING:root:Failure to load the inference.so custom c++ tensorflow ops. This error is likely caused the version of TensorFlow and TensorFlow Decision Forests are not compatible. Full error:/home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/tensorflow/ops/inference/inference.so: undefined symbol: _ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceESt17basic_string_viewIcSt11char_traitsIcEEPSt6vectorINSt7__cxx1112basic_stringIcS5_SaIcEEESaISB_EE\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "/home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/tensorflow/ops/inference/inference.so: undefined symbol: _ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceESt17basic_string_viewIcSt11char_traitsIcEEPSt6vectorINSt7__cxx1112basic_stringIcS5_SaIcEEESaISB_EE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfdf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/__init__.py:64\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_version\n\u001b[1;32m     62\u001b[0m check_version\u001b[38;5;241m.\u001b[39mcheck_version(__version__, compatible_tf_versions)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_tree\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/keras/__init__.py:53\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Decision Forest in a Keras Model.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mUsage example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrappers\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Utility classes\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/keras/core.py:61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tuner \u001b[38;5;28;01mas\u001b[39;00m tuner_lib\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_inference\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core \u001b[38;5;28;01mas\u001b[39;00m tf_core\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf1_compatibility\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/keras/core_inference.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_inference \u001b[38;5;28;01mas\u001b[39;00m tf_core\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m tf_op\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_learner_pb2\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultitasker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multitasker_pb2\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/tensorflow/ops/inference/api.py:179\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf1_compatibility\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_spec_pb2\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_model_pb2\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/tensorflow/ops/inference/op.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 Google LLC.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_dynamic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/tensorflow/ops/inference/op_dynamic.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Importing all the symbols.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/tensorflow/ops/inference/op_dynamic.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m   ops \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_op_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path_to_datafile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/framework/load_library.py:54\u001b[0m, in \u001b[0;36mload_op_library\u001b[0;34m(library_filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_op_library\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_op_library\u001b[39m(library_filename):\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a TensorFlow plugin, containing custom ops and kernels.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m  Pass \"library_filename\" to a platform-specific mechanism for dynamically\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    RuntimeError: when unable to load the library or get the python wrappers.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m   lib_handle \u001b[38;5;241m=\u001b[39m \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibrary_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     wrappers \u001b[38;5;241m=\u001b[39m _pywrap_python_op_gen\u001b[38;5;241m.\u001b[39mGetPythonWrappers(\n\u001b[1;32m     57\u001b[0m         py_tf\u001b[38;5;241m.\u001b[39mTF_GetOpList(lib_handle))\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages/tensorflow_decision_forests/tensorflow/ops/inference/inference.so: undefined symbol: _ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceESt17basic_string_viewIcSt11char_traitsIcEEPSt6vectorINSt7__cxx1112basic_stringIcS5_SaIcEEESaISB_EE"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1u-VD2KuKk0"
   },
   "source": [
    "## Get the data\n",
    "\n",
    "The Dataset is avalaible on [Kaggle](https://www.kaggle.com/c/nlp-getting-started)\n",
    "\n",
    "Dataset description:\n",
    "\n",
    "**Files:**\n",
    "\n",
    "- train.csv: the training set\n",
    "\n",
    "**Columns:**\n",
    "\n",
    "- id: a unique identifier for each tweet\n",
    "- text: the text of the tweet\n",
    "- location: the location the tweet was sent from (may be blank)\n",
    "- keyword: a particular keyword from the tweet (may be blank)\n",
    "- target: in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKnVT33PuKk0"
   },
   "outputs": [],
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/IMvision12/Tweets-Classification-NLP/main/train.csv\"\n",
    ")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKxAVK9vuKk0"
   },
   "source": [
    "The dataset includes 7613 samples with 5 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQ0F_vZ6uKk1"
   },
   "outputs": [],
   "source": [
    "print(f\"Training dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnc654t7uKk1"
   },
   "source": [
    "Shuffling and dropping unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pOYOjAMuKk1"
   },
   "outputs": [],
   "source": [
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "# Dropping id, keyword and location columns as these columns consists of mostly nan values\n",
    "# we will be using only text and target columns\n",
    "df_shuffled.drop([\"id\", \"keyword\", \"location\"], axis=1, inplace=True)\n",
    "df_shuffled.reset_index(inplace=True, drop=True)\n",
    "print(df_shuffled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDUmUu5WuKk1"
   },
   "source": [
    "Printing information about the shuffled dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l93EOLW-uKk1"
   },
   "outputs": [],
   "source": [
    "print(df_shuffled.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FOrE5QQuKk1"
   },
   "source": [
    "Total number of \"disaster\" and \"non-disaster\" tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xgi3k4qAuKk2"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Total Number of disaster and non-disaster tweets: \"\n",
    "    f\"{df_shuffled.target.value_counts()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKPpoXvPuKk2"
   },
   "source": [
    "Let's preview a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QseWCAH5uKk2"
   },
   "outputs": [],
   "source": [
    "for index, example in df_shuffled[:5].iterrows():\n",
    "    print(f\"Example #{index}\")\n",
    "    print(f\"\\tTarget : {example['target']}\")\n",
    "    print(f\"\\tText : {example['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBQHkwLWuKk2"
   },
   "source": [
    "Splitting dataset into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOjSCwQMuKk2"
   },
   "outputs": [],
   "source": [
    "test_df = df_shuffled.sample(frac=0.1, random_state=42)\n",
    "train_df = df_shuffled.drop(test_df.index)\n",
    "print(f\"Using {len(train_df)} samples for training and {len(test_df)} for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Xhl91NGuKk2"
   },
   "source": [
    "Total number of \"disaster\" and \"non-disaster\" tweets in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC02r-XyuKk2"
   },
   "outputs": [],
   "source": [
    "print(train_df[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR2RC7A5uKk2"
   },
   "source": [
    "Total number of \"disaster\" and \"non-disaster\" tweets in the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfaSE4MKuKk2"
   },
   "outputs": [],
   "source": [
    "print(test_df[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c0qnZiEuKk2"
   },
   "source": [
    "## Convert data to a `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmZ1495puKk2"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(dataframe):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dataframe[\"text\"].to_numpy(), dataframe[\"target\"].to_numpy())\n",
    "    )\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_ds = create_dataset(train_df)\n",
    "test_ds = create_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P1vNqa2uKk3"
   },
   "source": [
    "## Downloading pretrained embeddings\n",
    "\n",
    "The Universal Sentence Encoder embeddings encode text into high-dimensional vectors that can be\n",
    "used for text classification, semantic similarity, clustering and other natural language\n",
    "tasks. They're trained on a variety of data sources and a variety of tasks. Their input is\n",
    "variable-length English text and their output is a 512 dimensional vector.\n",
    "\n",
    "To learn more about these pretrained embeddings, see\n",
    "[Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihZkWp7yuKk3"
   },
   "outputs": [],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8Fu7k2OuKk3"
   },
   "source": [
    "## Creating our models\n",
    "\n",
    "We create two models. In the first model (model_1) raw text will be first encoded via\n",
    "pretrained embeddings and then passed to a Gradient Boosted Tree model for\n",
    "classification. In the second model (model_2) raw text will be directly passed to\n",
    "the Gradient Boosted Trees model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM5XY32cuKk3"
   },
   "source": [
    "Building model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4PuMhRCuKk3"
   },
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(), dtype=tf.string)\n",
    "outputs = sentence_encoder_layer(inputs)\n",
    "preprocessor = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_1 = tfdf.keras.GradientBoostedTreesModel(preprocessing=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FlAK9FwuKk3"
   },
   "source": [
    "Building model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmfQooTSuKk3"
   },
   "outputs": [],
   "source": [
    "model_2 = tfdf.keras.GradientBoostedTreesModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCgHqb2TuKk3"
   },
   "source": [
    "## Train the models\n",
    "\n",
    "We compile our model by passing the metrics `Accuracy`, `Recall`, `Precision` and\n",
    "`AUC`. When it comes to the loss, TF-DF automatically detects the best loss for the task\n",
    "(Classification or regression). It is printed in the model summary.\n",
    "\n",
    "Also, because they're batch-training models rather than mini-batch gradient descent models,\n",
    "TF-DF models do not need a validation dataset to monitor overfitting, or to stop\n",
    "training early. Some algorithms do not use a validation dataset (e.g. Random Forest)\n",
    "while some others do (e.g. Gradient Boosted Trees). If a validation dataset is\n",
    "needed, it will be extracted automatically from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2FyJHCUuKk3"
   },
   "outputs": [],
   "source": [
    "# Compiling model_1\n",
    "model_1.compile(metrics=[\"Accuracy\", \"Recall\", \"Precision\", \"AUC\"])\n",
    "# Here we do not specify epochs as, TF-DF trains exactly one epoch of the dataset\n",
    "model_1.fit(train_ds)\n",
    "\n",
    "# Compiling model_2\n",
    "model_2.compile(metrics=[\"Accuracy\", \"Recall\", \"Precision\", \"AUC\"])\n",
    "# Here we do not specify epochs as, TF-DF trains exactly one epoch of the dataset\n",
    "model_2.fit(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPYiMO_juKk3"
   },
   "source": [
    "Prints training logs of model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-u_ykAFuKk3"
   },
   "outputs": [],
   "source": [
    "logs_1 = model_1.make_inspector().training_logs()\n",
    "print(logs_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJf1GTFpuKk3"
   },
   "source": [
    "Prints training logs of model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ebLaa64uKk3"
   },
   "outputs": [],
   "source": [
    "logs_2 = model_2.make_inspector().training_logs()\n",
    "print(logs_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37F94_-HuKk3"
   },
   "source": [
    "The model.summary() method prints a variety of information about your decision tree model, including model type, task, input features, and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liT2Q537uKk8"
   },
   "outputs": [],
   "source": [
    "print(\"model_1 summary: \")\n",
    "print(model_1.summary())\n",
    "print()\n",
    "print(\"model_2 summary: \")\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcHHkzFiuKk8"
   },
   "source": [
    "## Plotting training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFzUUwntuKk8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_curve(logs):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
    "    plt.xlabel(\"Number of trees\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
    "    plt.xlabel(\"Number of trees\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_curve(logs_1)\n",
    "plot_curve(logs_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Fcl0jsfuKk8"
   },
   "source": [
    "## Evaluating on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tvgFJj_uKk8"
   },
   "outputs": [],
   "source": [
    "results = model_1.evaluate(test_ds, return_dict=True, verbose=0)\n",
    "print(\"model_1 Evaluation: \\n\")\n",
    "for name, value in results.items():\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "results = model_2.evaluate(test_ds, return_dict=True, verbose=0)\n",
    "print(\"model_2 Evaluation: \\n\")\n",
    "for name, value in results.items():\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yrg5-EBKuKk9"
   },
   "source": [
    "## Predicting on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RmgCu2cuKk9"
   },
   "outputs": [],
   "source": [
    "test_df.reset_index(inplace=True, drop=True)\n",
    "for index, row in test_df.iterrows():\n",
    "    text = tf.expand_dims(row[\"text\"], axis=0)\n",
    "    preds = model_1.predict_step(text)\n",
    "    preds = tf.squeeze(tf.round(preds))\n",
    "    print(f\"Text: {row['text']}\")\n",
    "    print(f\"Prediction: {int(preds)}\")\n",
    "    print(f\"Ground Truth : {row['target']}\")\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Q9DWbGuKk9"
   },
   "source": [
    "## Concluding remarks\n",
    "\n",
    "The TensorFlow Decision Forests package provides powerful models\n",
    "that work especially well with structured data. In our experiments,\n",
    "the Gradient Boosted Tree model with pretrained embeddings achieved 81.6%\n",
    "test accuracy while the plain Gradient Boosted Tree model had 54.4% accuracy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tweet-classification-using-tfdf",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
